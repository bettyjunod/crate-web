title: Crate guest post for Azure - Setting up Azure for Crate
author: Chris Ward
description: tbc
created: 2016-02-04
status: publish
post_type: post
tags: a, b
category: developernews

# Crate guest post for Azure - Setting up Azure for Crate

Crate.io has been striving to create the most scalable database in our industry. We aim to make scaling your data from millions to billions of records and beyond, as simple as possible.

We have clients who write and query gigabytes of data a minute, store terabytes a day and love the performance and stability that Crate offers. But the question always remained at the back of our minds, when taken to the extreme, what is Crate really capable of?

Our aims were to:

- See how Crate would distribute shards and partitions across such a large cluster
- Examine the overhead added by a the cluster management
- See what speeds the cluster could write data at
- Another
- And another

Microsoft Azure were generously lent us the use of 8000 cores as the infrastructure for this test and engineer time to guide us through problems and questions we might have.

Microsoft hoped to prove that their Azure platform was reliable enough for a project of such rapid scaling and resource intensive.

So, what happened and how did we do it?

We aimed to create a 1001 node Crate cluster and xxx. Setting up such a cluster on any kind of infrastructure requires planning and whilst cloud hosts simplify this process, understanding how to translate your requirements into their paradigms requires research and experimentation.

Azure has several concepts that we needed to enable and configure to get our cluster started:

- **Resource Group**: A 'container' that holds related resources of all types for an application.
- **Storage Account**: A data storage solution allocated to you that can contain Blob, Table, Queue, and File data. Can be stored on SSDs or HDDs.
- **Virtual Network**: A representation of a network in the cloud, where you can define DHCP address blocks, DNS settings, security policies, and routing.
- **Subnet**: Further segmentation for virtual networks.
- **Virtual Machine (VM)**: Represents a particular operating system or application stack.
- **Network Interface (NIC)**: Represents a network interface that can be associated to one or more virtual machines (VM).
- **Network Security Group (NSG)**: Contains a list of rules that allow or deny network traffic to your VMs in a Virtual Network. Can be associated with subnets or individual VM instances within that subnet.

## Data Set

70 terabytes of compressed data from the [common crawl database](http://commoncrawl.org/the-data/).

## Plan A

Our initial approach was to create a series of scripts to do setup the following:

1. A 3 node [Consul](https://www.consul.io/) cluster to manage the cluster and handle service discovery.
2. 3 Docker swarm master nodes to create VM instances quickly via docker machine.
3. Gradually scale the cluster by creating docker instances running Crate and managed by the Swarm master.
4. A 3 node monitoring and metrics cluster, we settled on [Ruxit](https://ruxit.com/).
5. Import our data set using Crate's `Copy From` command.

After a two day coding session with Microsoft staff in Vienna, they recommended we instead switched to using [Azure Templates](https://azure.microsoft.com/en-us/documentation/articles/resource-group-authoring-templates/), a json formatted file that provisions all the resources and commands to be run for your application in a single operation. These templates can be used in conjunction with the Azure command line tool to rigger a complex cluster quickly, for example:

```bash
azure group deployment create -f azuredeploy.json \
  -e azuredeploy.parameters.json \
  -g <resourcegroup>
```

For the convenience of our team and to allow for reproduction of individual clusters, we broke these templates into cluster functions. I.e. one template for creating the network infrastructure, one for installing Docker components etc.

### Problems

Plan A started well, but after a few hundred nodes we started to run into problems from different sources. First the network overhead of running traffic through the [Docker container bridge](https://docs.docker.com/engine/userguide/networking/dockernetworks/#a-bridge-network) proved to much and the Microsoft engineers recommended we switched to running Crate from Linux packages. Next there were problems with the Consul cluster as we had not configured it optimally to run on a multi-core machine and so it was struggling to meet the demand put upon it. After many hours debugging leading to a small config change we were able to get Consul running using all cores and reacting far more responsively. Finally despite being a great service, the Ruxit service couldn't cope with the traffic we were generating and the 29 metrics we wanted to collect. At around 500 nodes we were hitting their API so hard it shut down, so this wasn't a viable option for us, or for Ruxit.

## Plan B

With lessons learned it was time to make a few changes and start again, to summarize, this is what we changed:

- Replaced Ruxit with [Ganglia](http://ganglia.info/) for monitoring the cluster.
- Launch Crate from Linux packages instead of Docker containers

### Problems

The main issues encountered with this plan was the shear amount of connections and network traffic generated in a cluster of this size. For example, if the Consul, Docker Swarm or Crate master switches to a different node because of an issue, it will inform every node in the cluster, generating a lot of traffic across the network. The engineers at Microsoft were able to help us with this issue by changing some settings in the VM image kernel (**What was this?**) which improved cluster stability.

This improved speeds and network stability and so we started to import our data set and hit upon a so far uncovered bug with our `COPY FROM` command. Whilst the core team worked on solving this bug, we switched to using the [GitHub archive data set](https://www.githubarchive.org/) which offered more flexibility for import options so we could keep pushing forwards.

## Next Steps
- RRD data extraction to analyze main issues between stability.
- complete allocation of shards and partitions and cluster overhead.
- How many shards on a node equals what in terms of performance.
- Crate Master switched a lot, why?

We can't say this was production ready and made lots of queries, no, not possible. Wasn't stable enough.
