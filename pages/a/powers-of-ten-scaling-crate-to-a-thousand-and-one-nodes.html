title: Powers of Ten: Scaling Crate to a Thousand and One Nodes
link: https://crate.io/a/powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes
author: Nils Magnus
description: Crate teams up with Docker and Microsoft to see how well a database scales beyond 1000 nodes
created: 2016/03/11 12:12:43
post_name: powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes
status: publish
post_type: post
tags: cluster, azure, docker, swarm, iaas, import, virtual machines
category: news, developernews

![Create Virtual Network]({% media '/media/1603/stack-of-crates.png' %})

Our engineers wondered what it would be like to build an active-active database cluster comprising of a huge number of nodes. How would we design such a cluster, how would we deploy it and load data to its nodes while keeping a constant eye on its health? Using Microsoft Azure and Docker Swarm we've been able to set up a 1001 node cluster with a Petabyte capacity that is searchable in realtime. While the challenge was easy to define, design issues needed to be addressed. A cluster of that size was new to everyone in the team. The cluster runs in [Microsoft's IaaS service, Azure](https://crate.io/a/setting-up-azure-for-crate), which was also new to the team. In a previous stage we managed to set up a Docker Swarm cluster.

## A Special Ratio of Containers and VMs

Once the infrastructure layer was operational, we designed the container layer and decided to start with an unusual configuration. We planned to have only one container per virtual machine. We discussed whether having Docker as an extra layer would add more complexity to the setup, but agreed that the easier deployment and flexibility for different setups was worth the extra layer.

Crate database instances make good use of all available resources like memory, CPU cores, and fast storage (preferably SSDs). Crate's core is multithreaded and the JVM will happily use up to 32GB of RAM. Only on bigger machines it may (or may not) make sense to have more than a single instance running. As we decided to use 28GB of RAM per VM, we planned a single Crate instance on a host.

To set up the 1001-node-cluster, we split into teams. One working on the infrastructure layer, one preparing the orchestration and one taking care of the workload. Due to the abstraction the orchestration team was able to test Swarm even at a point when the infrastructure team did not have all VMs provisioned. As a test they dropped the one-container-one-VM rule and could test their scripts.

Deploying the first containers, we ran the docker command on the individual hosts. Doing this a couple of times made it clear that this was not an option. Once the Docker Swarm master was running, it automatically distributes the Docker commands issued across the cluster.

Next, we verified that the components of the Docker Swarm cluster worked as expected. Soon, we had a cluster of almost 1000 Azure VMs acting as Docker hosts and the same number of Crate instances. We attached two 512GB and one  256GB SSD locally redundant premium storage (LRS) to each VM, resulting in more than a Petabyte of gross capacity. Each host had nominal 5.100 IOPS and a data bandwidth of 400MB/s for the virtual hard discs (VHDs) [according to the Azure specs](https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage-preview-portal/).

## Downloading the Equivalent of 1,400 Bluray Disks

With the cluster ready, next we had to assign its workload. We had 70TB of compressed text data from the public [Common Crawl](http://commoncrawl.org/) project resulting in about 170TB text files. Don't underestimate transferring that amount of data across the Internet! We wrote a scheduler that parsed file lists of the Open Data project and fetched more than half a million files in the compressed [WARC](http://commoncrawl.org/2014/04/navigating-the-warc-file-format/) format. We used only the WET part of this archive file, that includes metadata and a flattened text version of the websites content.

To run a series of different imports, we decided to download the complete dataset (extending over a period of more than two years) and store it to standard LRS blob storage. Limitations of the individual data paths of the single VMs became obvious quickly. The throughput of a single worker collecting the data (via `wget`) and storing it to the blob storage (via `azure storage XXXXX XXXXX`) was too slow. At first we suspected the Internet connection between the two data centers of being the bottleneck, but learned that having more workers in parallel accelerated the download significantly. With a slower start, it took roughly XXX hours to download nearly 70TB compressed data.

With that amount of data, divided up into half a million of 100 MB chunks, connection disruption occurs occasionally. TCP connections get out of order or stall, the routing may change during a data transfer or other esoteric situations may happen. We ended up scripting our own monitoring, but agreed that for anything beyond a proof of concept, a more robust approach is desirable.

## Feeding the Monster

Next we had to deal with about 170TB of compressed text data. The WET format has a couple of key-value-pairs and the text equivalent of lots of webpages the spider collected:

```bash
WARC/1.0
WARC-Type: conversion
WARC-Target-URI: http://0x20.be/smw/index.php?title=Special:WhatLinksHere/CA&hidetrans=1
WARC-Date: 2015-04-18T03:22:32Z
WARC-Record-ID: <urn:uuid:535e1598-ed3e-4c4b-8365-6891fd4bfa3e>
WARC-Refers-To: <urn:uuid:78fdafb7-d7ec-44e8-8d43-85a1662baae3>
WARC-Block-Digest: sha1:7SBKQMNURN434CZR55HEKV4G5ESKBO6Z
Content-Type: text/plain
Content-Length: 1314

Pages that link to "CA" - Whitespace (Hackerspace Gent)
Pages that link to "CA"
<- CA
Jump to:        navigation, search
What links here
Page: Namespace: all
[...]
```

We estimated more than 30 billion records to load into the Crate cluster. Instead of transforming these records into a sequence of `INSERT` statements, we wrote a plugin that uncompresses the data on the fly, transforms it into JSON format and ingests the records directly, using our efficient `COPY FROM` command. Importing all records into an otherwise empty cluster took XXXXX hours. The import benefits from partitioning the data by timestamp:

```sql
create table if not exists commoncrawl (
  ssl boolean primary key,       -- http/https - byte/boolean primary key
  authority string primary key,  -- xyz.hello.com:123 primary key
  path string primary key,       -- /a?d=1#hello primary key
  date timestamp primary key,
  week_partition as date_trunc('week', date) primary key,
  --
  ctype string,
  clen int,
  content string INDEX using fulltext with (max_token_length = 40)
) clustered into 3 shards partitioned by (week_partition);
```

Since our data set contained timestamps and we were interested in how certain metrics developed over time, we decided to partition the schema into weeks. Once the node has determined the `week_partition` attribute, it routes the actual data to its final destination node. In production environments you are able to distribute this step to every node.

## Tuning the Cluster

While Crate and our Docker image use powerful defaults, settings for the storage strategy can be tuned for an extra boost. We tweaked these settings in the _crate.yml_ file:

```yaml
cluster.name: crate-swarm
indices.store.throttle.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 15%
bootstrap.mlockall: true
index.store.type: mmapfs
node.master: <master role>
node.data: <data node role>
transport.publish_host: <name>
multicast.enabled: false
discovery.zen.minimum_master_nodes: 2
discovery.type: srv
discovery.srv.query: <discovery url>
```

Variables like `<master role>` get substituted by the deployment script, implementing basic service discovery. Throttling the indices with 200MB/s is an unlikely real world value, but since we had all the hardware we could dream of, we decided to go crazy.

It was a wild journey. There were moments when things stopped working that worked before, or when things we tried didn't match expectations.

As tired eyes hack at lines of code whilst the sun sets over the mountains of Dornbirn and the grey streets of Berlin, it all seemed worth it. With the benefit of hindsight and the worst behind us, stress testing the tool you have been building for years is always a satisfying experience.

We will keep you posted and let us know about your own experiences and stories when a crazy idea came together.
