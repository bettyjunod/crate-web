title: Powers of Ten: Scaling SQL Database Crate to a Thousand and One Nodes
link: https://crate.io/a/powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes/
author: Nils Magnus
description: Crate teams up with Docker and Microsoft to see how well a database scales beyond 1000 nodes
created: 2016/03/10 12:12:43
post_name: powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes
status: publish
post_type: post
tags: cluster, azure, docker, swarm, iaas, petabyte, terabyte, import, node, vm, instance, template 
category: news, developernews

![Create Virtual Network]({% media '/media/1603/stack-of-crates.png' %})

Our engineers at Crate wondered what it's like to build an
active-active database cluster comprising of a huge number of
nodes. How would we design such a cluster, how would we deploy it,
load data to its nodes while keeping a constant eye on its health?
Facilitating Microsoft Azure and Docker Swarm we've been able to set
up a 1001 node cluster with a Petabyte capacity that is searchable in
realtime.  While the challenge was easy to define, a number of design
issues needed to be addressed. A cluster of that size was absolutely
new to everyone in the team. The cluster runs in [Microsoft's IaaS
service Azure](https://crate.io/a/setting-up-azure-for-crate). In a
previous stage we managed to set up Docker Swarm cluster.

# A special ratio of containers and VMs

Once the infrastructure layer was operational, we designed the
container layer and decided to start with a somewhat unusual
configuration: We planned to have only one container per virtual
machine. We discussed whether having Docker as an extra layer would
just add more complexity to the setup, but agreed that the easier
deployment and flexibility for different setups was worth the extra
layer.

Crate database instances can make good use of all available ressources
like memory, CPU cores, and fast storage (preferrable SSDs). Its core
is multithreaded and the JVM happily uses up to 32 GByte of RAM. Only
on bigger machines it may (or may not) make sense to have more than a
single instance running. As we decided to utilize 28 GByte or RAM per
VM, we planned only with a single Crate instance on a host.

To set up the 1001-node-cluster, we split up into several teams: One
working on the infrastructure layer, one preparing the orchestration
and one taking care of the workload. Due to the abstraction the
orchestration team was able to test Swarm even at a point when the
infrastructure team did not have all VMs provisioned: As a test they
dropped the one-container-one-VM rule and could test their scripts.

Starting to deploy the first containers, we ran the docker command
manually on the individual hosts. Doing this a couple of times
unmistakably makes clear that this is not an option. So once the
Docker Swarm master was up an runnning, the software automatically
distributed the Docker commands issued on the master node to the
swarm's nodes. This step is completely transparent for the user, since
Docker detects this operational mode once the `SWARM_MASTER`
environment variable is set.

Once that worked, we verified the components of the Docker swarm
cluster worked as expected. After a while, we had a cluster of almost
1000 Azure VMs acting as Docker hosts and the same number of Crate
database instances. We attached two 512 GByte and one additional 256
GByte SSD powered locally redundant premium storage (LRS) to each VM,
resulting to more than a Petabyte of gross capacity. Each host had
nominal 5.100 IOPS and a data bandwidth of 400 MByte/s for the virtual
hard discs (VHDs) [according to the Azure
specs](https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage-preview-portal/).

# Downloading the equivalent of 1.400 Bluray disks

With the cluster ready, we had to assign its workload. We had a
dataset of 70 TByte compressed text data from the public accessible
Common Crawl project resulting in about 170 TByte text files. Don't
underestimate transferring that amounts of data across the
Internet. We wrote a scheduler that parsed file lists of the Open Data
project and fetched more than half a million files in compressed WET
format. The project stores the results of a crawl of a substantial
part of the world wide web in the Web Archive Format. We just used
only the WET part, that includes metadata and a flattened text version
of the websites content.

In order to run different series of imports, we decided to once
download the complete dataset (extending over a period of more than
two years) and store it to some Standard LRS blob storage. Eventually
limitations of the individual data paths of the single VMs became
obvious: The troughput of a single worker collecting the data (via
`wget`) and storing it to the blob storage (via `azure storage XXXXX
XXXXX`) was too slow. While we first suspected the Internet connection
between the two data centers of being the bottleneck, we learned that
having more workers in parallel accelerated the download
significantly. With a slower start, it took roughly XXX hours to
download nearly 70 TByte compressed data.

With that amount of data, divided up into half a million of 100 MByte
chunks, connection disruption occurs every now and then. TCP
connections get out of order, stalls, the routing may change during a
data transfer or other esoteric situations may happen. We ended up to
script our own monitoring, but agreed that for anything beyond such a
proof of concept, a somewhat more robust approach is desirable.

# Feeding the monster

Now we had to deal With roughly 170 TByte of text data in compressed form. The WET format has a couple of key-value-pairs and the text equivalent of lots of actual webpages the spider collected:

```
WARC/1.0
WARC-Type: conversion
WARC-Target-URI: http://0x20.be/smw/index.php?title=Special:WhatLinksHere/CA&hidetrans=1
WARC-Date: 2015-04-18T03:22:32Z
WARC-Record-ID: <urn:uuid:535e1598-ed3e-4c4b-8365-6891fd4bfa3e>
WARC-Refers-To: <urn:uuid:78fdafb7-d7ec-44e8-8d43-85a1662baae3>
WARC-Block-Digest: sha1:7SBKQMNURN434CZR55HEKV4G5ESKBO6Z
Content-Type: text/plain
Content-Length: 1314

Pages that link to "CA" - Whitespace (Hackerspace Gent)
Pages that link to "CA"
<- CA
Jump to:        navigation, search
What links here
Page: Namespace: all
[...]   
```

We estimated to have more than 30 billion records to load into the
Crate cluster. In order to do so, instead of transforming it to a
sequence of `INSERT` statements, we wrote a plugin that uncompresses
the data on the fly, transforms it into a JSON format and ingests the
records directly, facilitating our very efficient `COPY FROM`
command. Importing all records into an otherwise empty cluster took
XXXXX hours. The import benefits from the partitioning of the data by
the timestamp:

```sql
create table if not exists commoncrawl (
  ssl boolean primary key,       -- http/https - byte/boolean primary key
  authority string primary key,  -- xyz.hello.com:123 primary key
  path string primary key,       -- /a?d=1#hello primary key
  date timestamp primary key,
  week_partition as date_trunc('week', date) primary key,
  --
  ctype string,
  clen int,
  content string INDEX using fulltext with (max_token_length = 40)
) clustered into 3 shards partitioned by (week_partition);
```

Since our data set contained timestamps and we are interested in how
certain metrics developed over time, we decided to partition the
schema into weeks. Simply put that works like a materialized
view. Once the node had determined the `week_partition` attribute, it
routes the actual data to its final destination node. In production
environments you are able to distribute this step to every node.
   
# Tuning the cluster

While Crate in general and specifically our Docker container use
powerful defaults, details of the storage strategy can be tuned for
the extra boost. We tweaked these settings in the `crate.yml`:

```yaml
cluster.name: crate-swarm
indices.store.throttle.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 15%
bootstrap.mlockall: true
index.store.type: mmapfs
node.master: <master role>
node.data:   <data node role>
transport.publish_host: <name>
multicast.enabled: false
discovery.zen.minimum_master_nodes: 2
discovery.type: srv
discovery.srv.query: <discovery url>
```

The variables like `<master role>` get substituted by the deployment
script, implementing some basic service discovery. Throttling the
indices with 200 MByte/s is a very keen value for a real world
scenario, but since we've had all the hardware we could dream of, we
decided to go bold here.

It was a wild journey, but edging out the tool we were building for
years is always a very satisfying experience. At least when it's done
in the retrospective. But still, there are moments when things stopped
working that worked before, new stuff didn't hit the expectations and
eyes are just falling shut while hacking the last lines of code in the
light of rising sun behind the mountains of Dornbirn. We keep you
posted. Let us know about your own experience in crossing borders.
