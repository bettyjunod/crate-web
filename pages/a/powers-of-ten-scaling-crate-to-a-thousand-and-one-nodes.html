title: Powers of Ten: Scaling SQL Database Crate to a Thousand and One Nodes
link: https://crate.io/a/powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes/
author: Nils Magnus
description: Crate teams up with Docker and Microsoft to see how well a database scales beyond 1000 nodes
created: 2016/03/10 12:12:43
post_name: powers-of-ten-scaling-crate-to-a-thousand-and-one-nodes
status: publish
post_type: post
tags: cluster, azure, docker, swarm, iaas, petabyte, terabyte, import, node, vm, instance, template 
category: news, developernews

![Create Virtual Network]({% media '/media/1512/azure-vpc.png' %})

Our engineers at Crate wondered what it's like to build an
active-active database cluster comprising of a huge number of
nodes. How would we design such a cluster, how would we deploy it,
load data to its nodes while keeping a constant eye on its health?
Facilitating Microsoft Azure and Docker Swarm we've been able to set
up a 1001 node cluster with a Petabyte capacity that is searchable in
realtime.  While the challenge was easy to define, a number of design
issues needed to be addressed. A cluster of that size was absolutely
new to everyone in the team. The cluster runs in [Microsoft's IaaS
service Azure](https://crate.io/a/setting-up-azure-for-crate). In a
previous stage we managed to set up Docker Swarm cluster.

# A special ratio of containers and VMs

Once the infrastructure layer was operational, we designed the
container layer and decided to start with a somewhat unusual
configuration: We planned to have only one container per virtual
machine. We discussed whether having Docker as an extra layer would
just add more complexity to the setup, but agreed that the easier
deployment and flexibility for different setups was worth the extra
layer.

Crate database instances can make good use of all available ressources
like memory, CPU cores, and fast storage (preferrable SSDs). Its core
is multithreaded and the JVM happily uses up to 32 GByte of RAM. Only
on bigger machines it may (or may not) make sense to have more than a
single instance running. As we decided to utilize 28 GByte or RAM per
VM, we planned only with a single Crate instance on a host.

To set up the 1001-node-cluster, we split up into several teams: One
working on the infrastructure layer, one preparing the orchestration
and one taking care of the workload. Due to the abstraction the
orchestration team was able to test Swarm even at a point when the
infrastructure team did not have all VMs provisioned: As a test they
dropped the one-container-one-VM rule and could test their scripts.

Starting to deploy the first containers, we ran the docker command
manually on the individual hosts. Doing this a couple of times
unmistakably makes clear that this is not an option. So once the
Docker Swarm master was up an runnning, the software automatically
distributed the Docker commands issued on the master node to the
swarm's nodes. This step is completely transparent for the user, since
Docker detects this operational mode once the `SWARM_MASTER`
environment variable is set.

Once that worked, we verified the components of the Docker swarm
cluster worked as expected. After a while, we had a cluster of almost
1000 Azure VMs acting as Docker hosts and the same number of Crate
database instances. We attached two 512 GByte and one additional 256
GByte SSD powered locally redundant premium storage (LRS) to each VM,
resulting to more than a Petabyte of gross capacity. Each host had
nominal 5.100 IOPS and a data bandwidth of 400 MByte/s for the virtual
hard discs (VHDs) [according to the Azure
specs](https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage-preview-portal/).

# Downloading the equivalent of 1.400 Bluray disks

With the cluster ready, we had to assign its workload. We had a
dataset of 70 TByte compressed text data from the public accessible
Common Crawl project resulting in about 170 TByte text files. Don't
underestimate transferring that amounts of data across the
Internet. We wrote a scheduler that parsed file lists of the Open Data
project and fetched more than half a million files in compressed WET
format. The project stores the results of a crawl of a substantial
part of the world wide web in the Web Archive Format. We just used
only the WET part, that includes metadata and a flattened text version
of the websites content.

In order to run different series of imports, we decided to once
download the complete dataset (extending over a period of more than
two years) and store it to some Standard LRS blob storage. Eventually
limitations of the individual data paths of the single VMs became
obvious: The troughput of a single worker collecting the data (via
`wget`) and storing it to the blob storage (via `azure storage XXXXX
XXXXX`) was too slow. While we first suspected the Internet connection
between the two data centers of being the bottleneck, we learned that
having more workers in parallel accelerated the download
significantly. With a slower start, it took roughly XXX hours to
download nearly 70 TByte compressed data.

With that amount of data, divided up into half a million of 100 MByte
chunks, connection disruption occurs every now and then. TCP
connections get out of order, stalls, the routing may change during a
data transfer or other esoteric situations may happen. We ended up to
script our own monitoring, but agreed that for anything beyond such a
proof of concept, a somewhat more robust approach is desirable.

# Feeding the monster

XXXX Since the data set was in a application specific format, we wrote
a Crate plugin, that allowed us to use our very efficient `COPY FROM`
command and directly converted the data while ingesting it..

XXXX We fed the data into XXXX nodes in parallel resulting in XXXXXX

While Crate itself uses powerful defaults, details of the storage
strategy can be tuned for the extra boost:

```yaml
cluster.name: crate-swarm
indices.store.throttle.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 15%
bootstrap.mlockall: true
index.store.type: mmapfs
node.master: <>
node.data: <>
http.port: 4200
transport.tcp.port: 4300
transport.publish_host: <name>
multicast.enabled: false discovery.zen.minimum_master_nodes: 2
discovery.type: srv
discovery.srv.query: azure1k.srv.fir.io
```

XXXX Since our data set contained timestamps and we are interested in
how certain metrics developed over time, we decided to partition the
schema into weeks. Simply put that works like a materialized view.

It was a wild journey, but edging out the tool we were building for
years is always a very satisfying experience. At least when it's done
**in** the *retrospective*. But still, there are moments when things stopped
working that worked before, new stuff didn't hit the expectations and
eyes are just falling shut while hacking the last lines of code in the
light of rising sun behind the mountains of Dornbirn.

Key takeaways are 1) loading data can take some time. 2)
unconventional designs sometimes make sense (similar numbers of hosts
and nodes), 3) proper advance planning affects the query speed.
